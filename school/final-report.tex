\documentclass[
	letterpaper,
	11pt
]{jdf}

\usepackage{graphicx}
\usepackage{booktabs} % For professional table lines (toprule,midrule,bottomrule)
\usepackage{longtable} % For tables that might span pages
\usepackage{array} % For custom table column types
\usepackage{enumitem} % For customizing lists
\usepackage[margin=1in]{geometry} % Ensure 1-inch margins

\addbibresource{references.bib} % Assuming references.bib exists in the same directory

% Define custom column types for the table
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\author{Alexander Litinsky}
\email{alitinsky3@gatech.edu}
\title{Gamified AI Tutor: Implementation Report}

\begin{document}

\maketitle

\begin{abstract}
This report details the implementation of a Gamified AI Tutor built with Next.js, TypeScript, Prisma, and PostgreSQL (via Supabase), integrated with OpenAI and Deepseek AI models. It covers the system architecture, implemented features (document processing, AI-generated study aids, SRS, and gamification), technical challenges, and future work. Core functionalities were successfully delivered within the January-April 2025 timeframe.
\end{abstract}

\section{Introduction}

The educational landscape is being transformed by AI, yet many existing tools lack effective personalization and motivation \cite{vanlehn2011, dominguez2013}. This project addresses these gaps by combining spaced repetition (SRS), AI-generated study aids, and gamification \cite{kapp2012, gibson2015}. The implemented system allows users to upload materials, receive AI-generated flashcards and quizzes, study using SRS, and stay motivated through points and achievements.

\section{Technology Stack}

Choosing the right technologies was crucial for building a robust, scalable, and maintainable application. I selected a modern, TypeScript-centric stack designed for full-stack development:

\begin{itemize}[noitemsep]
    \item \textbf{Core Framework:} Next.js 15 with React 18
    \item \textbf{API Layer:} Next.js type-safe API routes
    \item \textbf{Database:} PostgreSQL via Supabase with Prisma ORM
    \item \textbf{State Management:} Zustand
    \item \textbf{UI:} Shadcn/UI with Tailwind CSS
    \item \textbf{AI:} OpenAI and Deepseek APIs
    \item \textbf{Storage:} Supabase Storage
    \item \textbf{Background Jobs:} QStash
    \item \textbf{Deployment:} Vercel
\end{itemize}

\section{System Architecture}

I designed the application architecture to be modular and scalable, leveraging the capabilities of the Next.js framework and the chosen stack.

\subsection{Frontend Architecture}
Built with React components organized by feature (`src/components/`), using Next.js App Router for routing and Zustand for state management. TypeScript enforced type safety while Tailwind CSS and Shadcn/UI handled styling.

\subsection{Backend Architecture}
API routes (`src/app/api/`) handled requests while service modules (`src/lib/`) contained core logic for SRS, quizzes, achievements, and deck processing. These interacted with the database via Prisma.

\subsection{Database Architecture}
PostgreSQL schema managed by Prisma, with models for users, study materials, flashcards, quizzes, progress tracking, and achievements.

\subsection{AI Integration Architecture}
Backend modules (`src/lib/ai/`) handled prompt construction, API calls to OpenAI/Deepseek, and response parsing for tasks like flashcard generation and quiz creation.

\subsection{Asynchronous Processing Flow (QStash)}
Handling large PDF uploads required an asynchronous approach to avoid Vercel's serverless function timeouts. The flow worked as follows:
\begin{enumerate}[noitemsep, topsep=0pt]
    \item User uploads PDF via the frontend UI.
    \item Frontend calls an API (e.g., `startDeckProcessing`).
    \item This mutation performs initial validation, potentially uploads the file to Supabase Storage, and then publishes a message to a QStash queue. The message contains necessary context like the file path/ID and user ID.
    \item The mutation immediately returns a success response to the frontend, indicating processing has begun in the background.
    \item A separate API endpoint, designated as the QStash subscriber URL, receives the job message from the queue.
    \item This subscriber endpoint executes the long-running tasks: fetching the file, extracting text using `pdf-parse`, chunking the content, making potentially multiple sequential AI calls for analysis and study aid generation, and finally saving all results (`StudyMaterial`, `StudyContent`, `FlashcardContent`, etc.) to the database via Prisma.
    \item Upon completion (or failure), this background job updates the status of the `Deck` in the database. The frontend could poll this status or use a notification system (like Supabase Realtime, though not explicitly confirmed) to inform the user.
\end{enumerate}
This architecture ensured that the user interface remained responsive and that intensive processing could complete reliably.

\section{Feature Implementation Details}

My development efforts focused on delivering the most critical features proposed, ensuring a functional core learning loop.

\subsection{Document Processing \& Deck Creation}
\textbf{Status: Fully Implemented}

Users can upload PDFs/text files which are processed asynchronously via QStash:
\begin{itemize}[noitemsep]
    \item Files stored in Supabase Storage
    \item PDF text extracted using pdf-parse
    \item AI identifies key concepts and chunks content
    \item Organized into decks with optional tags
\end{itemize}

\subsection{AI-Generated Study Aids}
\textbf{Status: Partially Implemented}

\begin{itemize}[noitemsep]
    \item \textbf{Flashcards:} AI-generated term/definition pairs
    \item \textbf{Quizzes:} Multiple choice and free response questions
    \item \textbf{Notes/Mind Maps:} Deferred for future implementation
\end{itemize}

\subsection{Quiz System}
\textbf{Status: Fully Implemented}

Features include:
\begin{itemize}[noitemsep]
    \item Multiple choice and free response questions
    \item Customizable quiz configuration
    \item Basic difficulty adaptation
    \item Automatic scoring and feedback
    \item Performance analytics
\end{itemize}

\subsection{Spaced Repetition System (SRS)}
\textit{Proposal Goal: Implied through personalized learning paths and knowledge retention focus.}

\textbf{Implementation Status: Fully Implemented.}

Essential for durable learning, the SRS is a core implemented feature:
\begin{itemize}
    \item \textbf{Core Algorithm:} I implemented an SRS algorithm (likely inspired by SM-2) within `src/lib/srs.ts`. This logic takes past review history and user feedback to determine future scheduling.
    \item \textbf{Feedback Mechanism:} During study sessions (`src/components/session/`), users rate their recall difficulty for each flashcard (e.g., "Again," "Hard," "Good," "Easy") using dedicated buttons (`StudyActionButtons.tsx`).
    \item \textbf{Interaction Logging:} This feedback is logged in the `CardInteraction` table, providing the necessary input for the SRS algorithm.
    \item \textbf{Scheduling Logic:} Based on the interaction history and feedback, the algorithm calculates the next optimal review date and updates the card's scheduling parameters (interval, ease factor), likely stored within `UserProgress` or a related table.
    \item \textbf{Study Interface:} The frontend (`FlashcardContainer.tsx`) presents users only with cards that are currently "due" for review according to the schedule, ensuring efficient study time. `useStudySessionStore.ts` manages the state for these sessions.
\end{itemize}

\subsection{Gamification Elements}
\textit{Proposal Goal: Enhance engagement with points, badges, progress banners, streaks, and challenges.}

\textbf{Implementation Status: Fully Implemented.}

To make studying more engaging and rewarding, I integrated several gamification mechanics:
\begin{itemize}
    \item \textbf{Points System:} Users earn points for completing study activities like reviewing flashcards or finishing quizzes. Scores are likely tracked in `UserProgress`.
    \item \textbf{Achievements System:} A comprehensive system with 12 defined achievements (`Achievement` model, seeded in `prisma/seed.ts`) covering various milestones (e.g., "First Deck Created", "Quiz Novice", "Weekly Warrior", "Perfect Streak", "Master Scholar"). Each achievement has a unique SVG icon (`public/achievements/`). The system tracks earned achievements per user (`UserAchievement`) using logic in `src/lib/achievements.ts` and notifies users upon unlocking (`AchievementNotification.tsx`). Progress is visualized in the UI (`src/components/achievements/`).
    \item \textbf{Study Streaks:} The system tracks consecutive days or weeks of study activity, contributing to points and potentially unlocking specific achievements. This likely involves analyzing `StudySession` timestamps, possibly via a cron job (`src/app/api/cron/`).
    \item \textbf{Visual Feedback:} Progress bars (`AchievementProgressBar.tsx`), achievement displays, and potentially point counters provide ongoing visual feedback on accomplishments.
\end{itemize}
These elements aim to foster motivation and encourage consistent use of the application.
\newpage
\section{Comparison with Proposal}

Reflecting on my initial proposal, here’s a summary of how the final implementation aligns:

\begin{table}[h]
	\caption{Proposed vs. Implemented Features}
	\small \centering
	\begin{tabular}{L{0.3\linewidth} C{0.3\linewidth} L{0.3\linewidth}}
		\toprule[0.5pt]
		\textbf{Proposed Feature} & \textbf{Status} & \textbf{Notes} \\ \midrule
		Document Processing & Fully Implemented & PDF/text support, AI structuring, async handling via QStash. \\ \midrule
		Study Aid Generation & Partially Implemented & Flashcards \& Quizzes (MCQ/FRQ) generation fully functional. Structured Notes implicit. Mind Maps deferred. \\ \midrule
		Gamification Elements & Fully Implemented & Points, 12 achievement badges, streaks, progress visualization implemented. Daily challenges not explicit. \\ \midrule
        Spaced Repetition (Implied) & Fully Implemented & Core SRS algorithm, interaction tracking, and dedicated study session interface built. \\ \midrule
		User-Friendly Interface & Fully Implemented & Clean, responsive design using Next.js, Shadcn/UI, Tailwind CSS. \\ \midrule
		Chatbot Integration & Partially Implemented & Basic Q\&A functionality exists but lacks the proposed adaptive feedback and deep learning journey integration. \\ \midrule
		Personalized Learning Paths & Partially Implemented & Basic adaptive quiz difficulty implemented. Advanced knowledge tracing and adaptive hypermedia deferred. \\
		\bottomrule[0.5pt]
	\end{tabular}
\end{table}

\textbf{Commentary:} I successfully delivered the core user experience: uploading documents, getting AI study aids, learning with SRS, testing with quizzes, and being motivated by gamification. The quiz and achievement systems are particularly well-developed. As anticipated, the most complex AI features – deep personalization and a truly adaptive chatbot – were only partially implemented due to their significant development effort. Deferring Mind Maps was a necessary trade-off. The switch from Drizzle to Prisma ORM was a practical implementation choice. Overall, the project validates the core concept and provides a strong foundation.

\section{Ethical Considerations \& Data Privacy}

Building an AI-driven educational tool required careful consideration of ethical implications and data privacy.

\begin{itemize}
    \item \textbf{Data Privacy:} I leveraged Supabase's infrastructure, which offers standard security features. My database schema links user-specific data (progress, answers, interactions) directly to the authenticated user ID. While Supabase handles authentication securely, implementing explicit GDPR-style consent forms and detailed privacy policies would be necessary for a production application. User content (uploaded files, generated aids) is stored within the Supabase ecosystem.
    \item \textbf{AI Transparency \& Explainability:} Currently, the system doesn't explicitly label all AI-generated content or explain its reasoning (e.g., why a specific SRS interval was chosen). Future iterations could improve transparency by indicating AI involvement and potentially offering simplified explanations for AI-driven decisions, fostering user trust.
    \item \textbf{Bias Mitigation in AI:} LLMs can inherit biases. While designing prompts, I aimed for neutrality, but ensuring fairness and accuracy in generated flashcards and quiz questions across diverse topics requires ongoing monitoring, evaluation, and potentially bias detection techniques or prompt adjustments. Formal bias audits were outside the scope but would be important for wider deployment.
    \item \textbf{Responsible Gamification:} The gamification elements were designed to motivate learning based on established principles \cite{kapp2012, ryan2000}, focusing on rewarding effort and progress rather than creating addictive or manipulative loops. The goal was always to support, not supersede, the core learning objectives.
\end{itemize}
Maintaining ethical standards is an ongoing responsibility, requiring continuous evaluation and adaptation as the system evolves.

\section{Testing and Deployment}

\subsection{Testing Strategy}
While I didn't implement a comprehensive automated testing suite (like Jest/Vitest unit tests or Playwright/Cypress E2E tests) due to time constraints, I focused on ensuring quality through:
\begin{itemize}
    \item \textbf{Type Safety:} Leveraging TypeScript across the stack, along with Prisma's type-safe database client and end-to-end type safety, caught a significant number of potential errors during development.
    \item \textbf{Manual Testing:} I conducted thorough manual testing of all primary user flows, including various document upload scenarios, study aid generation, SRS session progression, quiz taking with different configurations, and achievement unlocking logic.
    \item \textbf{Linting/Formatting:} Using tools like ESLint and Prettier ensured code consistency and adherence to best practices.
\end{itemize}
Adding automated tests would be a high priority for future development to improve robustness and facilitate refactoring.

\subsection{Deployment Process}
I deployed the application to Vercel, which provided an optimized platform for Next.js applications:
\begin{itemize}
    \item \textbf{Infrastructure:} Vercel handled builds, global CDN deployment, serverless function execution for API routes, and static asset hosting.
    \item \textbf{Database Connectivity:} The deployed application connected securely to the Supabase PostgreSQL database using connection strings stored as environment variables in Vercel.
    \item \textbf{Secrets Management:} All sensitive keys (AI APIs, database credentials, QStash tokens) were managed securely through Vercel's environment variable system.
    \item \textbf{Continuous Deployment:} Vercel's Git integration automated deployments, triggering a new build and release whenever changes were pushed to the main branch of the repository (assumed GitHub).
\end{itemize}
This setup provided an efficient and scalable deployment solution.

\section{Key Challenges and Learnings}

This project presented several significant technical and design challenges, offering valuable learning experiences:

\begin{itemize}
    \item \textbf{Asynchronous Processing for AI Tasks:} The most critical challenge was handling the potentially long execution time of processing PDFs with multiple AI API calls within the constraints of Vercel's serverless functions. Synchronous processing led to frequent timeouts. Implementing an asynchronous architecture using QStash was the key solution. This involved designing idempotent job handlers, managing job state, and decoupling the intensive work from the user-facing request-response cycle. It underscored the importance of understanding platform limitations and choosing appropriate architectural patterns.
    \item \textbf{AI Prompt Engineering \& Output Parsing:} Getting reliable and structured output from LLMs (OpenAI, Deepseek) required considerable experimentation with prompt design. I needed to clearly define the desired output format (often JSON), provide few-shot examples in some cases, and implement robust parsing logic on the backend to handle variations or potential errors in the AI's response. Ensuring the quality and relevance of generated flashcards and quiz questions was an iterative process.
    \item \textbf{PDF Text Extraction Reliability:} PDFs are notoriously inconsistent. While `pdf-parse` worked well for many text-based PDFs, handling complex layouts, multi-column text, tables, or image-based PDFs reliably would require more advanced techniques (like layout analysis or OCR integration), which were beyond the scope of this project. I focused on handling common text-based PDF structures.
    \item \textbf{SRS Algorithm Implementation \& Tuning:} Implementing the core SRS scheduling logic based on user feedback was manageable, but fine-tuning the specific parameters (e.g., initial intervals, ease factor adjustments, handling lapses) for optimal learning across diverse subjects and users would ideally require A/B testing and analysis of real user data over time.
    \item \textbf{Full-Stack Type Safety Management:} While Prisma, and TypeScript provided excellent type safety, ensuring types were consistent across the frontend, backend API layer, database, and external API interactions required careful management and occasional type casting or validation.
\end{itemize}

My key takeaways include:
\begin{itemize}[noitemsep]
    \item Asynchronous processing is not optional but essential for building reliable applications involving potentially long-running tasks (like complex AI interactions) on serverless platforms.
    \item Type safety across the entire stack significantly improves developer productivity and reduces runtime errors.
    \item Integrating external AI APIs effectively is as much about prompt engineering, error handling, and output validation as it is about the API call itself.
    \item Modern frameworks like Next.js, combined with tools like Prisma provide a powerful and efficient environment for building complex full-stack applications.
\end{itemize}

\section{Future Work}

The current application serves as a strong foundation, but many exciting avenues exist for future development:

\begin{itemize}
    \item \textbf{Advanced Adaptive Learning:} Implement sophisticated knowledge tracing models (e.g., Bayesian Knowledge Tracing) by analyzing patterns in user quiz answers and SRS performance. Use this dynamic model of user mastery to truly personalize the learning path – adapting not just quiz difficulty but also suggesting specific content chunks to review, modifying SRS schedules more dynamically, and potentially tailoring explanations.
    \item \textbf{Context-Aware Chatbot Tutor:} Enhance the chatbot beyond simple Q\&A. Allow it to access the user's current context (e.g., the specific flashcard being viewed, recent quiz performance). Enable it to provide adaptive feedback ("I see you struggled with concept X on the last quiz, let's review it"), offer hints, explain concepts in different ways, and guide users more proactively.
    \item \textbf{Richer Content Analysis \& Understanding:} Improve the initial document processing pipeline. Use more advanced NLP techniques or AI prompts to better understand the semantic structure of the content, differentiate between definitions, examples, arguments, etc., and potentially identify prerequisite relationships between concepts within a document.
    \item \textbf{Expanded Content Input Options:} Add support for uploading other common document formats (e.g., DOCX, PPTX) using libraries like Mammoth or Apache POI (potentially run within the async job). Allow users to import content directly from web URLs or integrate with note-taking apps.
    \item \textbf{Enhanced User Analytics Dashboard:} Develop a dedicated analytics page providing users with detailed visualizations of their learning progress over time, mastery levels per topic or tag, quiz performance breakdowns, SRS statistics (e.g., retention rates), and study habit insights (e.g., session duration, consistency).
    \item \textbf{Collaborative Features \& Sharing:} Introduce functionality for users to share their created decks with others, potentially forming study groups or allowing peer review within the platform.
    \item \textbf{Refined AI and Cost Optimization:} Explore fine-tuning smaller, more specialized AI models for specific tasks (like flashcard generation) to potentially improve quality and reduce reliance on expensive general-purpose LLM APIs. Implement more sophisticated caching for AI results where appropriate.
\end{itemize}

\section{Conclusion}

In conclusion, the Gamified AI Tutor project successfully transitioned from its initial proposal to a functional and feature-rich application. It effectively demonstrates the potential of integrating user-provided content, AI-driven study aid generation, scientifically-backed learning techniques like Spaced Repetition, and engaging gamification elements. I successfully implemented the core user journey, allowing users to process their documents, generate flashcards and quizzes, study efficiently using SRS, and stay motivated through points and achievements.

The development process involved overcoming significant technical hurdles, most notably the need for an asynchronous architecture using QStash to handle intensive AI processing tasks reliably on a serverless platform. This, along with challenges in AI prompt engineering and PDF parsing, provided invaluable practical experience. The chosen technology stack, particularly the combination of Next.js, TypeScript, and Prisma, proved highly effective for building this complex full-stack application.

While advanced AI personalization features offer exciting avenues for future work, the current application provides substantial value as a personalized learning tool. It stands as a strong proof-of-concept, validating the core ideas behind the project and showcasing how modern web technologies and AI can be combined to create more effective, engaging, and personalized educational experiences. I am proud of the result and the learnings gained throughout this implementation journey.

\newpage
\section*{References}
\printbibliography[heading=none]

\end{document}